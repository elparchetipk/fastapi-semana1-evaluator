"""
Base evaluator class que sirve como foundation para todos los evaluadores semanales.
Proporciona funcionalidad com√∫n y define la interfaz que deben implementar los evaluadores espec√≠ficos.
"""
import os
import sys
import yaml
import json
import time
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime


class BaseEvaluator(ABC):
    """
    Clase base abstracta para todos los evaluadores semanales.
    
    Proporciona:
    - Carga de configuraci√≥n desde criteria.yaml
    - Pipeline com√∫n de evaluaci√≥n
    - Sistema de puntuaci√≥n configurable
    - Generaci√≥n de reportes
    - Manejo de errores estandarizado
    """
    
    def __init__(self, week_number: int, student_repo_path: str):
        """
        Inicializa el evaluador base.
        
        Args:
            week_number: N√∫mero de semana (1-11)
            student_repo_path: Ruta al repositorio del estudiante
        """
        self.week_number = week_number
        self.repo_path = Path(student_repo_path).resolve()
        self.week_dir = self._get_week_directory()
        
        # Cargar configuraci√≥n de la semana
        self.criteria = self._load_criteria()
        self.config = self.criteria.get('automation', {})
        
        # Resultados de evaluaci√≥n
        self.results = {}
        self.start_time = None
        self.end_time = None
        
        # Validar configuraci√≥n b√°sica
        self._validate_setup()
    
    def _get_week_directory(self) -> Path:
        """Obtiene el directorio de la semana actual"""
        base_dir = Path(__file__).parent.parent
        week_dir = base_dir / "weeks" / f"week{self.week_number:02d}"
        
        if not week_dir.exists():
            raise FileNotFoundError(f"Week directory not found: {week_dir}")
        
        return week_dir
    
    def _load_criteria(self) -> Dict[str, Any]:
        """Carga los criterios de evaluaci√≥n desde criteria.yaml"""
        criteria_file = self.week_dir / "criteria.yaml"
        
        if not criteria_file.exists():
            raise FileNotFoundError(f"Criteria file not found: {criteria_file}")
        
        try:
            with open(criteria_file, 'r', encoding='utf-8') as f:
                criteria = yaml.safe_load(f)
            
            # Validar estructura b√°sica
            required_keys = ['week_info', 'categories']
            for key in required_keys:
                if key not in criteria:
                    raise ValueError(f"Missing required key '{key}' in criteria.yaml")
            
            return criteria
            
        except yaml.YAMLError as e:
            raise ValueError(f"Invalid YAML in criteria file: {e}")
    
    def _validate_setup(self):
        """Valida que el setup b√°sico sea correcto"""
        # Verificar que el repositorio existe
        if not self.repo_path.exists():
            raise FileNotFoundError(f"Student repository not found: {self.repo_path}")
        
        # Verificar week_info
        week_info = self.criteria.get('week_info', {})
        if week_info.get('number') != self.week_number:
            raise ValueError(f"Week number mismatch: expected {self.week_number}, got {week_info.get('number')}")
    
    def evaluate(self) -> Dict[str, Any]:
        """
        Ejecuta el pipeline completo de evaluaci√≥n.
        
        Returns:
            Dict con resultados completos de evaluaci√≥n
        """
        self.start_time = datetime.now()
        
        try:
            # 1. Ejecutar checks comunes
            self.results.update(self._run_common_checks())
            
            # 2. Ejecutar checks espec√≠ficos de la semana
            specific_results = self.run_specific_checks()
            self.results.update(specific_results)
            
            # 3. Calcular puntuaci√≥n
            scoring = self._calculate_score()
            
            # 4. Generar reporte
            report = self._generate_report(scoring)
            
            # 5. Determinar si aprob√≥
            passed = self._determine_pass_status(scoring)
            
            self.end_time = datetime.now()
            
            # Extraer el score final para f√°cil acceso
            final_score = scoring.get('total', {})
            if isinstance(final_score, dict):
                final_score = final_score.get('percentage', 0)
            
            # Redondear score a 1 decimal para consistencia
            final_score = round(float(final_score), 1)
            
            return {
                "week": self.week_number,
                "student_repo": str(self.repo_path),
                "evaluation_time": self.end_time.isoformat(),
                "duration_seconds": (self.end_time - self.start_time).total_seconds(),
                "final_score": final_score,
                "results": self.results,
                "scoring": scoring,
                "report": report,
                "passed": passed,
                "passing_threshold": self.criteria['week_info'].get('passing_threshold', 70)
            }
            
        except Exception as e:
            self.end_time = datetime.now()
            return self._handle_evaluation_error(e)
    
    def _run_common_checks(self) -> Dict[str, Any]:
        """
        Ejecuta checks comunes que aplican a todas las semanas.
        Puede ser sobrescrito por evaluadores espec√≠ficos.
        """
        # Los evaluadores espec√≠ficos pueden implementar sus propios common checks
        # o usar CommonChecks del core
        try:
            from core.common_checks import CommonChecks
            common_checks = CommonChecks(self.repo_path)
            
            common_results = {}
            # Los common checks b√°sicos se pueden implementar aqu√≠ si es necesario
            # Por ahora, devolvemos un resultado vac√≠o para que no afecte la evaluaci√≥n
            return common_results
            
        except ImportError:
            # Si no hay common checks disponibles, devolver resultado vac√≠o
            return {}
    
    @abstractmethod
    def run_specific_checks(self) -> Dict[str, Any]:
        """
        Ejecuta verificaciones espec√≠ficas de la semana.
        Debe ser implementado por cada evaluador espec√≠fico.
        
        Returns:
            Dict con resultados de checks espec√≠ficos
        """
        pass
    
    def _calculate_score(self) -> Dict[str, Any]:
        """
        Calcula la puntuaci√≥n basada en los criterios configurados.
        """
        categories = self.criteria.get('categories', {})
        category_scores = {}
        total_possible = 0
        total_earned = 0
        
        for category_name, category_config in categories.items():
            category_weight = category_config.get('weight', 0)
            category_earned = self._calculate_category_score(category_name, category_config)
            
            category_scores[category_name] = {
                "earned": category_earned,
                "possible": category_weight,
                "percentage": (category_earned / category_weight * 100) if category_weight > 0 else 0
            }
            
            total_possible += category_weight
            total_earned += category_earned
        
        return {
            "categories": category_scores,
            "total": {
                "earned": total_earned,
                "possible": total_possible,
                "percentage": (total_earned / total_possible * 100) if total_possible > 0 else 0
            },
            "breakdown": {cat: scores["earned"] for cat, scores in category_scores.items()}
        }
    
    def _calculate_category_score(self, category_name: str, category_config: Dict) -> float:
        """
        Calcula la puntuaci√≥n para una categor√≠a espec√≠fica.
        
        Args:
            category_name: Nombre de la categor√≠a
            category_config: Configuraci√≥n de la categor√≠a
            
        Returns:
            Puntuaci√≥n obtenida para la categor√≠a
        """
        checks = category_config.get('checks', [])
        earned_points = 0
        
        for check in checks:
            check_name = check.get('name')
            check_points = check.get('points', 0)
            check_required = check.get('required', True)
            
            # Obtener resultado del check
            check_result = self._get_check_result(check_name)
            
            if check_result:
                earned_points += check_points
            elif check_required:
                # Si es requerido y fall√≥, penalizaci√≥n completa
                pass  # 0 puntos
            else:
                # Si es opcional y fall√≥, penalizaci√≥n parcial
                earned_points += check_points * 0.1  # 10% por el intento
        
        # No exceder el peso m√°ximo de la categor√≠a
        max_points = category_config.get('weight', 0)
        return min(earned_points, max_points)
    
    def _get_check_result(self, check_name: str) -> bool:
        """
        Obtiene el resultado de un check espec√≠fico desde los resultados.
        
        Args:
            check_name: Nombre del check a evaluar
            
        Returns:
            True si el check pas√≥, False si fall√≥
        """
        # Mapeo de nombres de checks a rutas en results
        check_mappings = {
            # Checks de estructura (Week 1)
            "requirements_txt": lambda r: r.get("structure", {}).get("files", {}).get("requirements_txt", False),
            "main_py_exists": lambda r: r.get("structure", {}).get("files", {}).get("main_py", False),
            "readme_exists": lambda r: r.get("structure", {}).get("files", {}).get("readme_md", False),
            
            # Checks de dependencias (Week 1)
            "fastapi_dependency": lambda r: r.get("requirements", {}).get("fastapi", False),
            "uvicorn_dependency": lambda r: r.get("requirements", {}).get("uvicorn", False),
            
            # Checks de app (Week 1)
            "app_import": lambda r: r.get("app_import", {}).get("import_ok", False) and r.get("app_import", {}).get("has_app", False),
            
            # Checks de endpoints (Week 1)
            "root_endpoint": lambda r: r.get("endpoints", {}).get("root_working", False),
            "docs_accessible": lambda r: r.get("endpoints", {}).get("docs_accessible", False),
            "parametric_endpoint": lambda r: r.get("endpoints", {}).get("parametric_endpoint", False),
            
            # Checks de documentaci√≥n (Week 1)
            "json_responses": lambda r: r.get("endpoints", {}).get("root_returns_json", False),
            "readme_reflection": lambda r: r.get("readme", {}).get("has_reflection", False),
            "setup_commands": lambda r: r.get("readme", {}).get("has_commands", False),
            
            # Checks generales (Week 1)
            "project_structure": lambda r: r.get("structure", {}).get("ok", False),
            "code_quality": lambda r: r.get("app_import", {}).get("import_ok", False),
            
            # Week 3 mappings
            "sqlalchemy_dependencies": lambda r: r.get("sqlalchemy_dependencies", {}).get("passed", False),
            "database_connection": lambda r: r.get("database_connection", {}).get("passed", False),
            "models_definition": lambda r: r.get("models_definition", {}).get("passed", False),
            "create_with_db": lambda r: r.get("create_with_db", {}).get("passed", False),
            "read_with_db": lambda r: r.get("read_with_db", {}).get("passed", False),
            "update_with_db": lambda r: r.get("update_with_db", {}).get("passed", False),
            "delete_with_db": lambda r: r.get("delete_with_db", {}).get("passed", False),
            "relationships": lambda r: r.get("relationships", {}).get("passed", False),
            "constraints": lambda r: r.get("constraints", {}).get("passed", False),
            "migrations": lambda r: r.get("migrations", {}).get("passed", False),
            "session_handling": lambda r: r.get("session_handling", {}).get("passed", False),
            "connection_pooling": lambda r: r.get("connection_pooling", {}).get("passed", False),
        }
        
        # Buscar el mapping correspondiente
        if check_name in check_mappings:
            try:
                return check_mappings[check_name](self.results)
            except (KeyError, TypeError):
                return False
        
        # Si no hay mapping espec√≠fico, buscar directamente en results
        return self._search_result_by_name(check_name)
    
    def _search_result_by_name(self, check_name: str) -> bool:
        """
        Busca un resultado por nombre en toda la estructura de results.
        """
        def search_recursive(obj, target):
            if isinstance(obj, dict):
                if target in obj:
                    return bool(obj[target])
                for value in obj.values():
                    result = search_recursive(value, target)
                    if result is not None:
                        return result
            return None
        
        result = search_recursive(self.results, check_name)
        return result if result is not None else False
    
    def _generate_report(self, scoring: Dict[str, Any]) -> str:
        """
        Genera el reporte final de evaluaci√≥n.
        
        Args:
            scoring: Resultados de puntuaci√≥n
            
        Returns:
            Reporte formateado en Markdown
        """
        try:
            # Intentar cargar template personalizado
            template_path = self.week_dir / "templates" / "feedback.md"
            if template_path.exists():
                return self._generate_custom_report(template_path, scoring)
        except Exception:
            pass
        
        # Fallback a reporte b√°sico
        return self._generate_basic_report(scoring)
    
    def _generate_custom_report(self, template_path: Path, scoring: Dict[str, Any]) -> str:
        """Genera reporte usando template personalizado"""
        try:
            with open(template_path, 'r', encoding='utf-8') as f:
                template = f.read()
            
            # Variables para el template
            week_info = self.criteria.get('week_info', {})
            total_score = scoring['total']['earned']
            max_score = scoring['total']['possible']
            passed = total_score >= self.criteria['week_info'].get('passing_threshold', 70)
            
            # Feedback espec√≠fico
            positive_feedback = self._generate_positive_feedback()
            improvement_feedback = self._generate_improvement_feedback()
            next_steps = self._generate_next_steps()
            
            # Reemplazar variables en el template
            template = template.format(
                status="‚úÖ APROBADO" if passed else "üïê PENDIENTE",
                total_score=int(total_score),
                pass_status="APROBADO" if passed else "PENDIENTE",
                # Scores por categor√≠a
                **{f"{cat}_score": int(scores['earned']) for cat, scores in scoring['categories'].items()},
                **{f"{cat}_status": "‚úÖ" if scores['earned'] >= scores['possible'] * 0.7 else "‚ö†Ô∏è" for cat, scores in scoring['categories'].items()},
                # Feedback sections
                positive_feedback=positive_feedback,
                improvement_feedback=improvement_feedback,
                next_steps=next_steps,
                evaluation_date=datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            )
            
            return template
            
        except Exception as e:
            return self._generate_basic_report(scoring)
    
    def _generate_basic_report(self, scoring: Dict[str, Any]) -> str:
        """Genera un reporte b√°sico si no hay template personalizado"""
        week_info = self.criteria.get('week_info', {})
        total_score = scoring['total']['earned']
        threshold = self.criteria['week_info'].get('passing_threshold', 70)
        passed = total_score >= threshold
        
        status = "‚úÖ **APROBADO**" if passed else "üïê **PENDIENTE**"
        
        report = f"""{status} ‚Äî Puntaje: **{int(total_score)}/100**

## Week {self.week_number}: {week_info.get('title', 'Evaluaci√≥n')}

### üìä Desglose por categor√≠as

| Categor√≠a | Puntaje | Estado |
|-----------|---------|---------|
"""
        
        for cat_name, cat_scores in scoring['categories'].items():
            earned = int(cat_scores['earned'])
            possible = int(cat_scores['possible'])
            status_icon = "‚úÖ" if earned >= possible * 0.7 else "‚ö†Ô∏è"
            report += f"| {cat_name.title()} | {earned}/{possible} | {status_icon} |\n"
        
        report += f"""
### üéØ Umbral de aprobaci√≥n: {threshold}%

### üìã Feedback espec√≠fico:
{self._generate_improvement_feedback()}

---
> ü§ñ Evaluaci√≥n autom√°tica generada el {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
"""
        
        return report
    
    def _generate_positive_feedback(self) -> str:
        """Genera feedback positivo basado en resultados"""
        feedback = []
        
        if self.results.get("app_import", {}).get("import_ok"):
            feedback.append("‚úÖ Tu aplicaci√≥n FastAPI se importa correctamente")
        
        if self.results.get("endpoints", {}).get("root_working"):
            feedback.append("‚úÖ El endpoint ra√≠z (/) est√° funcionando")
        
        if self.results.get("endpoints", {}).get("docs_accessible"):
            feedback.append("‚úÖ La documentaci√≥n autom√°tica (/docs) es accesible")
        
        if self.results.get("structure", {}).get("ok"):
            feedback.append("‚úÖ La estructura del proyecto es correcta")
        
        return "\n".join(feedback) if feedback else "Contin√∫a trabajando en los aspectos fundamentales."
    
    def _generate_improvement_feedback(self) -> str:
        """Genera feedback estructurado de mejora basado en checks"""
        
        # Separar elementos exitosos y fallidos
        successful_items = []
        improvement_items = []
        
        # Analizar resultados para identificar √©xitos y mejoras
        self._analyze_results_for_feedback(successful_items, improvement_items)
        
        # Construir feedback estructurado
        feedback_parts = []
        
        if successful_items:
            feedback_parts.append("‚úÖ **Lo que est√° bien:**")
            for i, item in enumerate(successful_items, 1):
                feedback_parts.append(f"{i}. {item}")
            feedback_parts.append("")  # L√≠nea en blanco
        
        if improvement_items:
            feedback_parts.append("üîß **Lo que se debe mejorar:**")
            for i, item in enumerate(improvement_items, 1):
                feedback_parts.append(f"{i}. {item}")
        
        if not improvement_items and successful_items:
            feedback_parts.append("üéâ **¬°Excelente trabajo!** No hay mejoras cr√≠ticas necesarias.")
        
        # Agregar feedback espec√≠fico de la semana si existe
        try:
            specific_feedback = self.get_week_specific_feedback(self.results)
            if specific_feedback:
                feedback_parts.append("")
                feedback_parts.extend(specific_feedback)  # Agregar directamente, ya que viene formateado
        except:
            pass
        
        return "\n".join(feedback_parts)
    
    def _generate_next_steps(self) -> str:
        """Genera recomendaciones para pr√≥ximos pasos"""
        if self.week_number < 11:
            next_week = self.week_number + 1
            return f"üöÄ Prep√°rate para la Week {next_week}. Revisa los materiales de la siguiente semana."
        else:
            return "üéì ¬°Felicitaciones! Has completado el bootcamp de FastAPI."
    
    def get_week_specific_feedback(self, results: Dict[str, Any]) -> List[str]:
        """
        Genera feedback espec√≠fico de la semana.
        Puede ser implementado por evaluadores espec√≠ficos.
        
        Args:
            results: Resultados de evaluaci√≥n
            
        Returns:
            Lista de mensajes de feedback espec√≠ficos
        """
        return []
    
    def _determine_pass_status(self, scoring: Dict[str, Any]) -> bool:
        """Determina si el estudiante aprob√≥ basado en la puntuaci√≥n"""
        threshold = self.criteria['week_info'].get('passing_threshold', 70)
        return scoring['total']['earned'] >= threshold
    
    def _handle_evaluation_error(self, error: Exception) -> Dict[str, Any]:
        """Maneja errores durante la evaluaci√≥n"""
        return {
            "week": self.week_number,
            "student_repo": str(self.repo_path),
            "evaluation_time": datetime.now().isoformat(),
            "error": True,
            "error_type": type(error).__name__,
            "error_message": str(error),
            "passed": False,
            "results": {},
            "scoring": {"total": {"earned": 0, "possible": 100}},
            "report": f"‚ùå **ERROR EN EVALUACI√ìN**\n\nTipo: {type(error).__name__}\nMensaje: {str(error)}\n\nContacta al instructor para revisi√≥n manual."
        }
    
    def get_criteria(self) -> Dict[str, Any]:
        """Retorna los criterios de evaluaci√≥n cargados"""
        return self.criteria
    
    def get_results(self) -> Dict[str, Any]:
        """Retorna los resultados de evaluaci√≥n"""
        return self.results
    
    def _analyze_results_for_feedback(self, successful_items: List[str], improvement_items: List[str]) -> None:
        """Analiza los resultados para categorizar elementos exitosos y mejoras necesarias"""
        
        # Analizar estructura de archivos
        file_structure = self.results.get("file_structure", {})
        if file_structure.get("main.py", False):
            successful_items.append("Archivo main.py presente y accesible")
        else:
            improvement_items.append("Crear archivo main.py en la ra√≠z del proyecto")
            
        if file_structure.get("requirements.txt", False):
            successful_items.append("Archivo requirements.txt presente")
        else:
            improvement_items.append("Crear archivo requirements.txt con dependencias")
            
        if file_structure.get("README.md", False):
            successful_items.append("Documentaci√≥n README.md presente")
        else:
            improvement_items.append("Crear archivo README.md con documentaci√≥n del proyecto")
        
        # Analizar dependencias
        dependencies = self.results.get("dependencies", {})
        if dependencies.get("fastapi", False):
            successful_items.append("FastAPI correctamente instalado y configurado")
        else:
            improvement_items.append("Agregar 'fastapi' a requirements.txt")
            
        if dependencies.get("uvicorn", False):
            successful_items.append("Uvicorn disponible para ejecutar la aplicaci√≥n")
        else:
            improvement_items.append("Agregar 'uvicorn' a requirements.txt para ejecutar la app")
        
        # Analizar sintaxis
        syntax_validation = self.results.get("syntax_validation", {})
        if syntax_validation:
            syntax_errors = []
            for file_name, syntax_result in syntax_validation.items():
                if isinstance(syntax_result, dict):
                    if syntax_result.get("syntax_valid", False):
                        successful_items.append(f"Sintaxis correcta en {file_name}")
                    else:
                        syntax_errors.append(file_name)
                        
            if syntax_errors:
                improvement_items.append(f"Corregir errores de sintaxis en: {', '.join(syntax_errors)}")
        
        # Analizar documentaci√≥n
        documentation = self.results.get("documentation", {})
        if isinstance(documentation, dict):
            if documentation.get("score", 0) > 0:
                successful_items.append("Documentaci√≥n b√°sica presente en README")
            if documentation.get("score", 0) >= 5:
                successful_items.append("README con buena estructura y contenido")
